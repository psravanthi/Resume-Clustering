{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import string\n",
    "from datetime import datetime\n",
    "import time \n",
    "import unicodedata\n",
    "import pandas as pd\n",
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read resumes from the site (indeed.com)\n",
    "def resume_parser(url):\n",
    "\n",
    "    response = requests.get(url)\n",
    "    page = response.text\n",
    "    soup = BeautifulSoup(page)\n",
    "    work_summary = []\n",
    "    edu_summary = []\n",
    "    Summary,skills,additional_info = [\"\"]*3\n",
    "    #div_name = soup.findAll('div',{\"id\" : \"basic_info_cell\",\"class\":\"data_display\"})\n",
    "    #name = div_name[0].findAll('h1',{\"id\" : \"resume-contact\",\"class\":\"fn\"})[0].text\n",
    "    if len(soup.findAll('div',{ \"id\" : \"work-experience-items\" })) > 0 :\n",
    "        div_work_items= soup.findAll('div',{ \"id\" : \"work-experience-items\" })\n",
    "        div_work = div_work_items[0].findAll('div',{\"class\" : \"work-experience-section \"})\n",
    "        div_work.append(div_work_items[0].findAll('div',{\"class\" : \"work-experience-section last\"})[0])\n",
    "        for i in range(len(div_work)):\n",
    "            \n",
    "            if len(div_work[i].findAll('p',{\"class\":\"work_title title\"})) > 0:\n",
    "                work_title = div_work[i].findAll('p',{\"class\":\"work_title title\"})[0].text\n",
    "            else:\n",
    "                 work_title = \"\"\n",
    "            if len(div_work[i].findAll('div',{\"class\" : \"work_company\"})) >0:\n",
    "                work_company = div_work[i].findAll('div',{\"class\" : \"work_company\"})[0].text\n",
    "            else:\n",
    "                work_company = \"\"\n",
    "            if len(div_work[i].findAll('p',{\"class\":\"work_dates\"})) > 0 :\n",
    "                work_dates = div_work[i].findAll('p',{\"class\":\"work_dates\"})[0].text\n",
    "            else:\n",
    "                work_dates = \"\"\n",
    "            if len(div_work[i].findAll('p',{\"class\":\"work_description\"})) > 0 :\n",
    "                work_ex = div_work[i].findAll('p',{\"class\":\"work_description\"})[0].text\n",
    "            else:\n",
    "                work_ex = \"\"\n",
    "            \n",
    "            work_summary.append([work_title,work_company,work_dates,work_ex])\n",
    "            \n",
    "            #print(name,work_title,\" \",work_company,\" \",work_dates,\" \",work_ex,\"\\n\")\n",
    "            \n",
    "    if len(soup.findAll('p',{\"id\" : \"res_summary\"})) > 0 :\n",
    "        Summary = soup.findAll('p',{\"id\" : \"res_summary\"})[0].text\n",
    "    \n",
    "    if len(soup.findAll('div',{ \"id\" : \"education-items\" })) > 0 :\n",
    "        div_edu_items= soup.findAll('div',{ \"id\" : \"education-items\" })\n",
    "        div_edu = div_edu_items[0].findAll('div',{\"class\" : \"education-section \"})\n",
    "        div_edu.append(div_edu_items[0].findAll('div',{\"class\" : \"education-section last\"})[0])\n",
    "        for i in range(len(div_edu)):\n",
    "\n",
    "            if len(div_edu[i].findAll('p',{\"class\" : \"edu_title\"})) > 0 :\n",
    "                edu_title = edu_title = div_edu[i].findAll('p',{\"class\" : \"edu_title\"})[0].text\n",
    "            else:\n",
    "                edu_title = \"\"\n",
    "            \n",
    "            if len(div_edu[i].findAll('div',{\"class\" : \"edu_school\"})) > 0:\n",
    "                edu_school= div_edu[i].findAll('div',{\"class\" : \"edu_school\"})[0].text\n",
    "            else:\n",
    "                edu_school= \"\"\n",
    "                \n",
    "            \n",
    "            \n",
    "            edu_summary.append([edu_title,edu_school])\n",
    "            #print(edu_title,\" \",edu_school,\" \",edu_dates,\"\\n\")\n",
    "    \n",
    "    if len(soup.findAll('div',{ \"class\" : \"skill-container resume-element\" })) > 0:\n",
    "        skills = soup.findAll('div',{ \"class\" : \"skill-container resume-element\" })[0].text\n",
    "        #print(skills,\"\\n\")\n",
    "        \n",
    "    if len(soup.findAll('div',{ \"id\" : \"additionalinfo-section\" })) > 0 :\n",
    "        additional_info = soup.findAll('div',{ \"id\" : \"additionalinfo-section\" })[0].text\n",
    "        #print(additional_info)\n",
    "    return work_summary,Summary,edu_summary,skills,additional_info\n",
    "    \n",
    "    \n",
    "#work_summary,Summary,edu_summary,skills,additional_info = resume_parser('http://www.indeed.com/r/Nan-Li/98159002e3b9337b')\n",
    "#print(name,\"Work Summary:\",work_summary,\"\\n\\n\",\"Summary:\",Summary,\"\\n\\n\",\"Education:\",edu_summary,\"\\n\\n\",\"Skills:\",skills,\"\\n\\n\",\"Additional Info\",additional_info)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Transform the unicode string \n",
    "def str_transform(raw_string):\n",
    "    translator = str.maketrans({key: None for key in string.punctuation})\n",
    "    raw_string = raw_string.translate(translator)\n",
    "    raw_string = unicodedata.normalize(\"NFKD\",raw_string ).encode('ascii', 'ignore').decode(\"utf-8\") \n",
    "    raw_string = raw_string.lower()\n",
    "    raw_string = ' '.join(raw_string.split())\n",
    "    \n",
    "    cleaned_string = ''.join([i for i in raw_string if not i.isdigit()])\n",
    "    return cleaned_string\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Clean the text(education and work experience) read from the site\n",
    "def cleaning(work_summary,Summary,edu_summary,skills,additional_info):\n",
    "    \n",
    "    clean_work_summary = []\n",
    "    clean_edu_summary = []\n",
    "    translator = str.maketrans({key: None for key in string.punctuation})\n",
    "    for i in range(len(work_summary)):\n",
    "        temp_item = work_summary[i]\n",
    "        for j in range(len(temp_item)):\n",
    "            temp_item[j] = str_transform(temp_item[j])\n",
    "\n",
    "        clean_work_summary.append(temp_item)\n",
    "    for i in range(len(edu_summary)-1):\n",
    "        temp_item = edu_summary[i]\n",
    "        for j in range(len(temp_item)-1):\n",
    "            temp_item[j] = str_transform(temp_item[j])\n",
    "        clean_edu_summary.append(temp_item)\n",
    "    Summary = str_transform(Summary)\n",
    "    skills = str_transform(skills)\n",
    "    additional_info = str_transform(additional_info)\n",
    "    return work_summary,Summary,edu_summary,skills,additional_info\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# List of page urls for data consultant\n",
    "page_url.append(\"http://www.indeed.com/resumes?q=analytics+consultant&l=\")\n",
    "url = 'http://www.indeed.com/resumes?q=analytics+consultant&co=US&start='\n",
    "for num in [50,100]:\n",
    "    page_url.append(url+str(num))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "page_url = []\n",
    "# List of page urls for data analytics project manager\n",
    "page_url.append(\"http://www.indeed.com/resumes?q=analytics+project+manager&l=\")\n",
    "url = 'http://www.indeed.com/resumes?q=analytics+project+manager&co=US&start='\n",
    "for num in [50,100]:\n",
    "    page_url.append(url+str(num))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# List of page urls for data engineer\n",
    "page_url.append(\"http://www.indeed.com/resumes?q=data+engineer&l=\")\n",
    "url = 'http://www.indeed.com/resumes?q=data+engineer&co=US&start='\n",
    "for num in [50,100]:\n",
    "    page_url.append(url+str(num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# List of page urls for data analyst\n",
    "page_url.append(\"http://www.indeed.com/resumes?q=data+analyst&l=\")\n",
    "url = 'http://www.indeed.com/resumes?q=data+analyst&co=US&start='\n",
    "for num in [50,100]:\n",
    "    page_url.append(url+str(num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# List of page urls for business data analyst\n",
    "page_url.append(\"http://www.indeed.com/resumes?q=business+data+analyst&l=\")\n",
    "url = 'http://www.indeed.com/resumes?q=business+data+analyst&co=US&start='\n",
    "for num in [50,100]:\n",
    "    page_url.append(url+str(num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "['http://www.indeed.com/resumes?q=analytics+project+manager&l=', 'http://www.indeed.com/resumes?q=analytics+project+manager&co=US&start=50', 'http://www.indeed.com/resumes?q=analytics+project+manager&co=US&start=100', 'http://www.indeed.com/resumes?q=data+engineer&l=', 'http://www.indeed.com/resumes?q=data+engineer&co=US&start=50', 'http://www.indeed.com/resumes?q=data+engineer&co=US&start=100', 'http://www.indeed.com/resumes?q=data+analyst&l=', 'http://www.indeed.com/resumes?q=data+analyst&co=US&start=50', 'http://www.indeed.com/resumes?q=data+analyst&co=US&start=100', 'http://www.indeed.com/resumes?q=business+data+analyst&l=', 'http://www.indeed.com/resumes?q=business+data+analyst&co=US&start=50', 'http://www.indeed.com/resumes?q=business+data+analyst&co=US&start=100', 'http://www.indeed.com/resumes?q=business+intelligence&l=', 'http://www.indeed.com/resumes?q=business+intelligence&co=US&start=50', 'http://www.indeed.com/resumes?q=business+intelligence&co=US&start=100']\n"
     ]
    }
   ],
   "source": [
    "# List of page urls for business intelligence\n",
    "page_url.append(\"http://www.indeed.com/resumes?q=business+intelligence&l=\")\n",
    "url = 'http://www.indeed.com/resumes?q=business+intelligence&co=US&start='\n",
    "for num in [50,100]:\n",
    "    page_url.append(url+str(num))\n",
    "print(len(page_url))\n",
    "print(page_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n"
     ]
    }
   ],
   "source": [
    "with open(\"links_list.csv\",'w') as myfile:\n",
    "    wr = csv.writer(myfile, quoting=csv.QUOTE_ALL)\n",
    "    wr.writerow(page_url)\n",
    "print(len(page_url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.indeed.com/resumes?q=attorney&l=\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Manoj/anaconda/lib/python3.5/site-packages/bs4/__init__.py:166: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "To get rid of this warning, change this:\n",
      "\n",
      " BeautifulSoup([your markup])\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup([your markup], \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    }
   ],
   "source": [
    "# Get the list of resume urls \n",
    "resumes = []\n",
    "site_url = \"http://www.indeed.com\"\n",
    "for page_index in page_url:\n",
    "    print(page_index)\n",
    "    response = requests.get(page_index)\n",
    "    page = response.text\n",
    "    soup = BeautifulSoup(page)\n",
    "    #page = open(page_index)\n",
    "    #soup = BeautifulSoup(page.read())\n",
    "    result_list = soup.findAll('ol',{ \"class\" : \"resultsList\" })\n",
    "    results = result_list[0].findAll('div',{ \"class\" : \"clickable_resume_card\" })\n",
    "    \n",
    "    for i in range(len(results)):\n",
    "        temp_url = results[i]['onclick'].split()[0]\n",
    "        temp_url = temp_url.replace(\"window.open('\",\"\")\n",
    "        url = temp_url[:temp_url.find(\"?\")]\n",
    "        resumes.append(site_url+url)\n",
    "\n",
    "import csv\n",
    "with open(\"resumes.csv\",'w') as myfile:\n",
    "    wr = csv.writer(myfile, quoting=csv.QUOTE_ALL)\n",
    "    wr.writerow(resumes)\n",
    "print(len(resumes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Scrape the resumes from indeed, clean and save the data in a dataframe\n",
    "resume_summary = []\n",
    "count = 0\n",
    "for res in resumes:\n",
    "    final_summary = {}\n",
    "    print(res)\n",
    "    work_summary,Summary,edu_summary,skills,additional_info = resume_parser(res)\n",
    "    work_summary,Summary,edu_summary,skills,additional_info = cleaning(work_summary,Summary,edu_summary,skills,additional_info)\n",
    "    #final_summary[\"name\"] = name\n",
    "    final_summary[\"work_summary\"] = work_summary\n",
    "    final_summary[\"Summary\"] = Summary\n",
    "    final_summary[\"edu_summary\"] = edu_summary\n",
    "    final_summary[\"skills\"] = skills\n",
    "    final_summary[\"additional_info\"] = additional_info\n",
    "    resume_summary.append(final_summary)\n",
    "    count = count + 1\n",
    "    if count%100 == 0:\n",
    "        print(\"Processed:\",count)\n",
    "#print(resume_summary)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Connect to MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pymongo import MongoClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "client = MongoClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "client.resumes_cluster.drop_collection('resumes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "resumes = client.resumes_cluster.resumes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Manoj/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:2: DeprecationWarning: insert is deprecated. Use insert_one or insert_many instead.\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "for item in resume_summary:\n",
    "    resumes.insert(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
